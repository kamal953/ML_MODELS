# 1. Bagging vs Boosting:
# Bagging trains models in parallel on bootstrapped samples to reduce variance.
# Boosting trains models sequentially, each correcting its predecessor, reducing bias and improving accuracy.

# 2. Random Forest reduces variance by building many decision trees on random subsets of data and features.
# Their uncorrelated predictions are averaged, stabilizing the final output and avoiding overfitting.

# 3. Boosting methods can be prone to overfitting, especially when focusing too heavily on misclassified noise.
# They are also more computationally intensive due to their sequential nature.
