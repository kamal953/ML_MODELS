# 1. Entropy measures the impurity or randomness in a dataset; higher entropy means more disorder.
#    Information gain quantifies the reduction in entropy after splitting the data on a feature.

# 2. Entropy uses logarithmic calculations and can be more computationally intensive.
#    Gini Index is faster and measures impurity based on squared probabilities; both help choose optimal splits.

# 3. A decision tree may overfit by growing too deep and learning noise in the training data.
#    Overfitting can be avoided using pruning, setting max_depth, or using techniques like cross-validation.
